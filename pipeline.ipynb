{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# TITLE OF PROJECT\n",
    "## Contributors:\n",
    "### Anik Burman, Indian Statistical Institute\n",
    "### Joshua Fink, University of Michigan\n",
    "### Sasha Lioutikova, Yale University\n",
    "### Grace Smith, William and Mary\n",
    "## Special Thanks:\n",
    "### Dr. Johann Gagnon-Bartsch, Juejue Wang, and Heather Johnston"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. PREPROCESSING CSV FILES WITH AND WITHOUT SYNONYM HASHMAP\n",
    "### Prior to running the analysis, clean the csv file to ensure the input works well. The functions generate a hashmap with the top 150 words, then it inserts them into the tweets in lexigraphical order"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "'''\n",
    "HELPER FUNCTIONS AND LIBRARIES, PREPROCESSING AND SYNONYMS\n",
    "'''\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import ftfy\n",
    "\n",
    "# maybe remove other punctuation (like | …) and emojis?\n",
    "## pre-processing: stem, remove URLs and stop words\n",
    "def preProcessingFcn(tweet, removeWords=list(), stem=False, removeURL=True, removeStopwords=True,\n",
    "\tremoveNumbers=False, removeHashtags=True, removeAt=True, removePunctuation=True, lem=True, removeNewline=True,\n",
    "    ftfyCleanup=True, removeB=True, replaceApostrophe=True, removeEmoji=True):\n",
    "        ps = PorterStemmer()\n",
    "        lm = WordNetLemmatizer()\n",
    "        tweet = tweet.lower()\n",
    "        if removeB==True:\n",
    "            tweet = re.sub(r\"b\", \" \", tweet)\n",
    "        if removeURL==True:\n",
    "            tweet = re.sub(r\"http\\S+\", \" \", tweet)\n",
    "        if removeHashtags==True:\n",
    "            tweet = tweet.replace('#', ' ')\n",
    "        if removeAt==True:\n",
    "            tweet = tweet.replace('@', ' ')\n",
    "        if removeNumbers==True:\n",
    "            tweet=  ''.join(i for i in tweet if not i.isdigit())\n",
    "        if removePunctuation==True:\n",
    "            tweet = re.sub(r\"[,.;@#?!&$:]+\\ *\", \" \", tweet)\n",
    "        if replaceApostrophe==True:\n",
    "            tweet = re.sub(r\"’\", \"'\", tweet)### Joshua Fink, University of Michigan\n",
    "        # if removeNewline==True:\n",
    "        #     tweet = re.sub('\\n', ' ', tweet)\n",
    "        # if ftfyCleanup==True:### Joshua Fink, University of Michigan\n",
    "        #     tweet = ftfy.fix_text(tweet)\n",
    "        if removeStopwords==True:\n",
    "            tweet = ' '.join([word for word in tweet.split() if word not in stopwords.words('english')])\n",
    "        if len(removeWords)>0:\n",
    "            tweet = ' '.join([word for word in tweet.split() if word not in removeWords])\n",
    "        if lem==True:\n",
    "            tweet = ' '.join([lm.lemmatize(word) for word in tweet.split()])\n",
    "        if stem==True:\n",
    "            tweet = ' '.join([ps.stem(word) for word in tweet.split()])\n",
    "        return tweet\n",
    "\n",
    "\n",
    "# credit @Abdul-Razak Adam on StackOverflow\n",
    "def deEmojify(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)\n",
    "\n",
    "\n",
    "def preProcessingFcnTotal(tweets, removeWords=list(), stem=False, removeURL=True, removeStopwords=True,\n",
    "\tremoveNumbers=False, removeHashtags=True, removeAt=True, removePunctuation=True, lem=True, removeNewline=True,\n",
    "    ftfyCleanup=True, removeB=True):\n",
    "    preprocessed_tweets = []\n",
    "    for tweet in tweets:\n",
    "        preprocessed_tweets.append(preProcessingFcn(tweet, removeWords, stem, removeURL, removeStopwords,removeNumbers,\n",
    "                                                    removeHashtags, removeAt, removePunctuation, lem, removeNewline,\n",
    "                                                    ftfyCleanup, removeB))\n",
    "    return preprocessed_tweets\n",
    "\n",
    "\n",
    "''' GETTING TOP WORDS '''\n",
    "'''\n",
    "IN: n = number of words to return, tweets = list of tweets\n",
    "OUT: list of top n most frequent words in corpus\n",
    "'''\n",
    "def get_top_n_words(n, tweets):\n",
    "    counter_obj = Counter()\n",
    "    for tweet in tweets:\n",
    "        counter_obj.update(word_tokenize(tweet))\n",
    "    n_most_frequent_wcount = counter_obj.most_common(n)  # n most frequent words with counts\n",
    "    n_most_frequent = [pair[0] for pair in n_most_frequent_wcount]  # n most frequent words (without counts)\n",
    "    # print(n_most_frequent_wcount)\n",
    "    # print(n_most_frequent)\n",
    "    return n_most_frequent\n",
    "\n",
    "\n",
    "''' MAKING HASH MAP '''\n",
    "# boolean determining whether to use only unigrams\n",
    "# currently should be True; non-unigrams are not yet included in synonym-based tweet altering\n",
    "only_unigrams = True\n",
    "\n",
    "'''\n",
    "IN: list of \"base\" words to consider in hash map\n",
    "OUT: hash map including all inputted base words where key:pair = synonym:base word\n",
    "note: using version 2 of add_to_mapping (i.e. no separation of finding and adding synonyms)\n",
    "'''\n",
    "def make_mapping(words):\n",
    "    mapping = {}\n",
    "    for word in words:\n",
    "        for syn in wordnet.synsets(word):\n",
    "            for lemma in syn.lemma_names():\n",
    "                if lemma not in mapping and lemma != word:\n",
    "                    if (only_unigrams and '_' not in lemma) or not only_unigrams:\n",
    "                        mapping[lemma] = word\n",
    "    return mapping\n",
    "\n",
    "\n",
    "''' ALTERING TWEETS '''\n",
    "'''\n",
    "IN: tweets = original corpus, mapping = synonym:base mapping\n",
    "OUT: altered tweets with all base words added to end of tweet if synonym found in tweet\n",
    "'''\n",
    "def alter_tweets(tweets, mapping):\n",
    "    new_tweets = []\n",
    "    for tweet in tweets:\n",
    "        for word in word_tokenize(tweet):\n",
    "            if word in mapping:\n",
    "                tweet = tweet + \" \" + mapping[word]\n",
    "        new_tweets.append(tweet)\n",
    "    return new_tweets"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "'''\n",
    "PROCESS CSV INPUTS\n",
    "'''\n",
    "\n",
    "''' READING IN DATA '''\n",
    "'''\n",
    "IN: filepath (string), column index with tweet text (int)\n",
    "OUT: tweets as list of strings\n",
    "'''\n",
    "def read_data(filepath, colind):\n",
    "    data = []\n",
    "    with open(filepath, newline=\"\", encoding='utf-8') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            data.append(row[colind])\n",
    "    return data[1:]\n",
    "\n",
    "\n",
    "''' WRITING TO CSV '''\n",
    "\n",
    "def write_data(filepath, tweets):\n",
    "    with open(filepath, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['text'])\n",
    "        for tw in tweets:\n",
    "            writer.writerow([tw])\n",
    "\n",
    "\n",
    "# read in data\n",
    "data_incentive = read_data('../grace-sandbox/Data/VT_incentive.csv', 1)\n",
    "#data_non_incentive = read_data('../grace-sandbox/Data/VT_non_incentive.csv', 1)\n",
    "data_mani = read_data('../grace-sandbox/Data/Twitter_mani.csv', 4)\n",
    "\n",
    "# print(data_incentive[:5])\n",
    "# print(data_non_incentive[:5])\n",
    "print(data_mani[:5])\n",
    "\n",
    "\n",
    "# preprocess\n",
    "# VT_incentive_clean = preProcessingFcnTotal(data_incentive)\n",
    "# VT_non_incentive_clean = preProcessingFcnTotal(data_non_incentive)\n",
    "mani_clean = preProcessingFcnTotal(data_mani, removeB=False)\n",
    "\n",
    "# print(VT_incentive_clean[:5])\n",
    "# print(VT_non_incentive_clean[:5])\n",
    "print(mani_clean[:5])\n",
    "\n",
    "\n",
    "# # alter w/ synonyms\n",
    "# VT_incentive_altered = alter_tweets(VT_incentive_clean, make_mapping(get_top_n_words(50, VT_incentive_clean)))\n",
    "# VT_non_incentive_altered = alter_tweets(VT_non_incentive_clean, make_mapping(get_top_n_words(50, VT_non_incentive_clean)))\n",
    "mani_altered = alter_tweets(mani_clean, make_mapping(get_top_n_words(150, mani_clean)))\n",
    "print(mani_altered[:5])\n",
    "\n",
    "\n",
    "# # output\n",
    "# write_data('./VT_incentive_clean.csv', VT_incentive_clean)\n",
    "# write_data('./VT_non_incentive_clean.csv', VT_non_incentive_clean)\n",
    "write_data('./mani_clean150.csv', mani_clean)\n",
    "# write_data('./VT_incentive_altered.csv', VT_incentive_altered)\n",
    "# write_data('./VT_non_incentive_altered.csv', VT_non_incentive_altered)\n",
    "write_data('./mani_altered150.csv', mani_altered)\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}