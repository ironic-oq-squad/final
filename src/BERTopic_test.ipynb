{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "!pip install bertopic\n",
    "import bertopic"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: bertopic in /home/joshfink/.local/lib/python3.8/site-packages (0.8.1)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.8/dist-packages (from bertopic) (1.3.0)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.8/dist-packages (from bertopic) (0.24.2)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.8/dist-packages (from bertopic) (4.61.2)\n",
      "Requirement already satisfied: sentence-transformers>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from bertopic) (2.0.0)\n",
      "Requirement already satisfied: hdbscan>=0.8.27 in /usr/local/lib/python3.8/dist-packages (from bertopic) (0.8.27)\n",
      "Requirement already satisfied: umap-learn>=0.5.0 in /usr/local/lib/python3.8/dist-packages (from bertopic) (0.5.1)\n",
      "Requirement already satisfied: plotly<4.14.3,>=4.7.0 in /usr/local/lib/python3.8/dist-packages (from bertopic) (4.14.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.8/dist-packages (from bertopic) (1.21.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/lib/python3/dist-packages (from pandas>=1.1.5->bertopic) (2.7.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/lib/python3/dist-packages (from pandas>=1.1.5->bertopic) (2019.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.22.2.post1->bertopic) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.22.2.post1->bertopic) (2.2.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.22.2.post1->bertopic) (1.7.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.1.96)\n",
      "Requirement already satisfied: torchvision in /home/joshfink/.local/lib/python3.8/site-packages (from sentence-transformers>=0.4.1->bertopic) (0.10.0+cpu)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/joshfink/.local/lib/python3.8/site-packages (from sentence-transformers>=0.4.1->bertopic) (1.9.0+cpu)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.8/dist-packages (from sentence-transformers>=0.4.1->bertopic) (0.0.14)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.8/dist-packages (from sentence-transformers>=0.4.1->bertopic) (4.8.2)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from sentence-transformers>=0.4.1->bertopic) (3.6.2)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from hdbscan>=0.8.27->bertopic) (1.14.0)\n",
      "Requirement already satisfied: cython>=0.27 in /usr/local/lib/python3.8/dist-packages (from hdbscan>=0.8.27->bertopic) (0.29.24)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.8/dist-packages (from umap-learn>=0.5.0->bertopic) (0.5.4)\n",
      "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.8/dist-packages (from umap-learn>=0.5.0->bertopic) (0.53.1)\n",
      "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.8/dist-packages (from plotly<4.14.3,>=4.7.0->bertopic) (1.3.3)\n",
      "Requirement already satisfied: pillow>=5.3.0 in /usr/lib/python3/dist-packages (from torchvision->sentence-transformers>=0.4.1->bertopic) (7.0.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.6.0->sentence-transformers>=0.4.1->bertopic) (3.10.0.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->sentence-transformers>=0.4.1->bertopic) (21.0)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from huggingface-hub->sentence-transformers>=0.4.1->bertopic) (2.22.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->sentence-transformers>=0.4.1->bertopic) (3.0.12)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (0.10.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2021.7.6)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (5.3.1)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.8/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (0.0.45)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk->sentence-transformers>=0.4.1->bertopic) (7.0)\n",
      "Requirement already satisfied: llvmlite>=0.30 in /usr/local/lib/python3.8/dist-packages (from pynndescent>=0.5->umap-learn>=0.5.0->bertopic) (0.36.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (57.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/lib/python3/dist-packages (from packaging>=20.9->huggingface-hub->sentence-transformers>=0.4.1->bertopic) (2.4.6)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import re\n",
    "import csv\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import random\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preProcessingFcn(tweet, removeWords=list(), stem=True, removeURL=True, removeStopwords=True,\n",
    "                     removeNumbers=True, removeHashtags=True, removeAt=True, removePunctuation=True, lem=False):\n",
    "    ps = PorterStemmer()\n",
    "    lm = WordNetLemmatizer()\n",
    "    tweet = tweet.lower()\n",
    "    if removeURL == True:\n",
    "        tweet = re.sub(r\"http\\S+\", \" \", tweet)\n",
    "    if removeHashtags == True:\n",
    "        tweet = tweet.replace('#', ' ')\n",
    "    if removeAt == True:\n",
    "        tweet = tweet.replace('@', ' ')\n",
    "    if removeNumbers == True:\n",
    "        tweet = ''.join(i for i in tweet if not i.isdigit())\n",
    "    if removePunctuation == True:\n",
    "        tweet = re.sub(r\"[,.;@#?!&$:]+\\ *\", \" \", tweet)\n",
    "    if removeStopwords == True:\n",
    "        tweet = ' '.join([word for word in tweet.split()\n",
    "                          if word not in stopwords.words('english')])\n",
    "    if len(removeWords) > 0:\n",
    "        tweet = ' '.join([word for word in tweet.split()\n",
    "                         if word not in removeWords])\n",
    "    if lem == True:\n",
    "        tweet = ' '.join([lm.lemmatize(word) for word in tweet.split()])\n",
    "    if stem == True:\n",
    "        tweet = ' '.join([ps.stem(word) for word in tweet.split()])\n",
    "    return tweet\n",
    "\n",
    "''' MAKING HASH MAP '''\n",
    "# boolean determining whether to use only unigrams\n",
    "# currently should be True; non-unigrams are not yet included in synonym-based tweet altering\n",
    "only_unigrams = True\n",
    "\n",
    "'''\n",
    "IN: list of \"base\" words to consider in hash map\n",
    "OUT: hash map including all inputted base words where key:pair = synonym:base word\n",
    "note: using version 2 of add_to_mapping (i.e. no separation of finding and adding synonyms)\n",
    "'''\n",
    "def make_mapping(words):\n",
    "    mapping = {}\n",
    "    for word in words:\n",
    "        for syn in wordnet.synsets(word):\n",
    "            for lemma in syn.lemma_names():\n",
    "                if lemma not in mapping and lemma != word:\n",
    "                    if (only_unigrams and '_' not in lemma) or not only_unigrams:\n",
    "                        mapping[lemma] = word\n",
    "    return mapping\n",
    "\n",
    "\n",
    "''' ALTERING TWEETS '''\n",
    "'''\n",
    "IN: tweets = original corpus, mapping = synonym:base mapping\n",
    "OUT: altered tweets with all base words added to end of tweet if synonym found in tweet\n",
    "'''\n",
    "def alter_tweets_with_ordering(tweets, mapping):\n",
    "    new_tweets = []\n",
    "    for tweet in tweets:\n",
    "        tweet_refactor = \"\"\n",
    "        for word in word_tokenize(tweet):\n",
    "            tweet_refactor = tweet_refactor + \" \" + word\n",
    "            if word in mapping:\n",
    "                tweet_refactor = tweet_refactor + \" \" + mapping[word]\n",
    "        tweet_refactor = tweet_refactor[1:]\n",
    "        new_tweets.append(tweet_refactor)\n",
    "    return new_tweets\n",
    "\n",
    "INPUT_DF = pd.read_csv('Vaccine_Trust_cleaned_nostop.csv')\n",
    "print(INPUT_DF)\n",
    "corpusLong = []\n",
    "for index, rows in INPUT_DF.iterrows():\n",
    "    sentence = preProcessingFcn(rows.text)\n",
    "    corpusLong.append(sentence)\n",
    "\n",
    "corpus = []\n",
    "corpus = corpusLong\n",
    "#corpus = random.sample(corpusLong, 500)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /home/joshfink/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/joshfink/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                                   text\n",
      "0     type guy wont take vaccine go 2 tin dip 6 ener...\n",
      "1     child many country tell get vaccine covidvacci...\n",
      "2     safe vaccine determine intelligent scientist c...\n",
      "3     prime minister publicly say didnt trust nvacci...\n",
      "4     act stupid old enough appreciate amaze medical...\n",
      "...                                                 ...\n",
      "9128  baylor religion survey find identify biblical ...\n",
      "9129  get friend take vaccine cuz trust like bitch w...\n",
      "9130  trust russian self drive car much trust russia...\n",
      "9131  leave incapable critical think nvirtue signal ...\n",
      "9132  sniff k yes idk like trust vaccine know litera...\n",
      "\n",
      "[9133 rows x 1 columns]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# LOADING MODELS\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load sentence transformer model\n",
    "sentence_model = SentenceTransformer(\"paraphrase-MiniLM-L6-v2\")\n",
    "\n",
    "# Create documents embeddings\n",
    "embeddings = sentence_model.encode(corpus, show_progress_bar=False)\n",
    "print(len(embeddings), \", \", len(embeddings[0]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "9133 ,  384\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import hdbscan\n",
    "import umap.umap_ as umap\n",
    "from bertopic import BERTopic\n",
    "\n",
    "# Define UMAP model to reduce embeddings dimension\n",
    "umap_model = umap.UMAP(n_neighbors=15,\n",
    "                       n_components=10,\n",
    "                       min_dist=0.0,\n",
    "                       low_memory=False)\n",
    "\n",
    "# Define HDBSCAN model to perform documents clustering\n",
    "hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=50,\n",
    "                                min_samples=1,\n",
    "                                #metric='jaccard',\n",
    "                                cluster_selection_method='eom',\n",
    "                                prediction_data=True)\n",
    "\n",
    "# Create BERTopic model\n",
    "topic_model = BERTopic(top_n_words=5,\n",
    "                       n_gram_range=(1,2), \n",
    "                       calculate_probabilities=True,\n",
    "                       umap_model= umap_model,\n",
    "                       hdbscan_model=hdbscan_model,\n",
    "                       verbose=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Train model, extract topics and probabilities\n",
    "topics, probabilities = topic_model.fit_transform(corpus, embeddings)\n",
    "\n",
    "count = 0\n",
    "total = 0\n",
    "for topic_elt in topics:\n",
    "    total = total + 1\n",
    "    if topic_elt == -1:\n",
    "        count = count + 1\n",
    "print(count)\n",
    "print(count/total*100)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-07-19 13:49:49,195 - BERTopic - Reduced dimensionality with UMAP\n",
      "2021-07-19 13:49:51,600 - BERTopic - Clustered UMAP embeddings with HDBSCAN\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3356\n",
      "36.745866637468524\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#ANIK CSV CODE\n",
    "umap_embedding = umap_model.fit_transform(embeddings)\n",
    "print( len(umap_embedding), \", \", len(umap_embedding[0]))\n",
    "UMAP_DF = pd.DataFrame(umap_embedding)\n",
    "UMAP_DF.to_csv(\"umap_9133_10.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "topic_model.visualize_distribution(probabilities[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "topic_model.visualize_heatmap()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "topic_model.visualize_topics()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(topic_model.get_topics())\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "frequency = topic_model.get_topic_freq()\n",
    "print(frequency)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(topic_model.get_topics())\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "topic_model.get_params()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# Vader Sentiment Analysis\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk import tokenize\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# An \"interface\" to matplotlib.axes.Axes.hist() method\n",
    "\n",
    "\n",
    "# sort tweets\n",
    "print(max(topics))\n",
    "\n",
    "tweet_clusters = {}\n",
    "for i in range(len(topics)):\n",
    "    if topics[i] != -1:\n",
    "        if topics[i] not in tweet_clusters:\n",
    "            tweet_clusters[topics[i]] = list()\n",
    "        tweet_clusters[topics[i]].append(corpus[i])\n",
    "\n",
    "\n",
    "for i in range(0,max(topics)+1):\n",
    "    \n",
    "    for elt in tweet_clusters[i]:\n",
    "        freq_ss = []\n",
    "        ss = sid.polarity_scores(elt)\n",
    "        freq_ss.append(ss[:, 'compound'])\n",
    "    print(\"Topic: \", i)\n",
    "    print(freq_ss)\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "49\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/joshfink/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "unhashable type: 'slice'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-cfe6aca90f5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mfreq_ss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolarity_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mfreq_ss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'compound'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     plt.hist(x=freq_ss, bins='auto', color='#0504aa',\n\u001b[1;32m     31\u001b[0m                             alpha=0.7, rwidth=0.85)\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'slice'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}